{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/amontoison/Workshop-GERAD/blob/main/gpu_kernels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjJTIwxxJjwX"
   },
   "source": [
    "# Parallel computing and GPU programming with Julia\n",
    "##Â Part IV: GPU Kernels with KernelAbstractions.jl\n",
    "Alexis Montoison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSgoDg8fFyEp",
    "outputId": "24d9fea9-a0f6-4ffc-81a1-6eda492ec0e4"
   },
   "outputs": [],
   "source": [
    "import Pkg\n",
    "Pkg.activate(\"colab6\")\n",
    "Pkg.add([\"CUDA\", \"KernelAbstractions\", \"Adapt\", \"NVTX\", \"ImageShow\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nJdnRZwNF2fh",
    "outputId": "3a36f4c1-e9e3-4957-c985-d9bf3882ba24"
   },
   "outputs": [],
   "source": [
    "versioninfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ru_R_unVF8JR"
   },
   "outputs": [],
   "source": [
    "using CUDA, KernelAbstractions, Adapt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJThsWC1Jjwk"
   },
   "source": [
    "### Different layers of abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "a = 0.5\n",
    "X_cpu = rand(Float64, N)\n",
    "Y_cpu = zeros(Float64, N)\n",
    "X = CuVector(X_gpu)\n",
    "Y = CuVector(Y_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fURgDPvJjwl"
   },
   "source": [
    "#### Vendor-specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8i8b3ZtJjwm"
   },
   "outputs": [],
   "source": [
    "function saxpy!(a,X,Y)\n",
    "    i = (blockIdx().x - 1) * blockDim().x + threadIdx().x\n",
    "    if i <= length(Y)\n",
    "        @inbounds Y[i] = a * X[i] + Y[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "@cuda threads=32 blocks=cld(length(Y), 32) saxpy!(a, X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dlA94N55Jjwn"
   },
   "source": [
    "#### KernelAbstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uLaCJWFYJjwo"
   },
   "outputs": [],
   "source": [
    "using KernelAbstractions\n",
    "using CUDA\n",
    "\n",
    "@kernel function saxpy!(a, @Const(X), Y)\n",
    "    I = @index(Global)\n",
    "    @inbounds Y[I] = a * X[I] + Y[I]\n",
    "end\n",
    "\n",
    "saxpy!(CUDABackend())(a, X, Y, ndrange=length(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Co2nDQkJjwr"
   },
   "source": [
    "#### Array abstractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-MD_tILRJjwr"
   },
   "outputs": [],
   "source": [
    "Y .= a .* X .+ Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KernelAbstractions.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Graphics/KA1.png' width='1000px'>\n",
    "<img src='./Graphics/KA2.png' width='1000px'>\n",
    "<img src='./Graphics/KA3.png' width='1000px'>\n",
    "<img src='./Graphics/KA4.png' width='1000px'>\n",
    "<img src='./Graphics/KA5.png' width='1000px'>\n",
    "<img src='./Graphics/KA6.png' width='1000px'>\n",
    "<img src='./Graphics/KA7.png' width='1000px'>\n",
    "<img src='./Graphics/KA8.png' width='1000px'>\n",
    "<img src='./Graphics/KA9.png' width='1000px'>\n",
    "<img src='./Graphics/KA10.png' width='1000px'>\n",
    "<img src='./Graphics/KA11.png' width='1000px'>\n",
    "<img src='./Graphics/KA12.png' width='1000px'>\n",
    "<img src='./Graphics/KA13.png' width='1000px'>\n",
    "<img src='./Graphics/KA14.png' width='1000px'>\n",
    "<img src='./Graphics/KA15.png' width='1000px'>\n",
    "<img src='./Graphics/KA16.png' width='1000px'>\n",
    "<img src='./Graphics/KA17.png' width='1000px'>\n",
    "<img src='./Graphics/KA18.png' width='1000px'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R-ZB0lDaJjwt"
   },
   "source": [
    "#### Summary -- How to use KernelAbstractions?\n",
    "\n",
    "- Use `@kernel function mykernel(args...) end` to write a GPU-style program\n",
    "- Instantiate kernel for a backend `kernel = mykernel(backend)`\n",
    "- Backends come from Vendor specific libraries\n",
    "- `KA.allocate(backend, ...)` to obtain memory\n",
    "- Launch kernel `kernel(args..., ndrange=...)` while specifying the grid to execute over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCjiqBQQJjwv"
   },
   "outputs": [],
   "source": [
    "function vadd(a, b, c)\n",
    "    for i in eachindex(c)\n",
    "        c[i] = a[i] + b[i]\n",
    "    end\n",
    "end\n",
    "\n",
    "a = rand(N)\n",
    "b = rand(N)\n",
    "c = similar(a)\n",
    "\n",
    "vadd(a, b, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zxhl2gcyJjwv"
   },
   "outputs": [],
   "source": [
    "import KernelAbstractions as KA\n",
    "\n",
    "@kernel function vadd(a, b, c)\n",
    "    i = @index(Global)\n",
    "    c[i] = a[i] + b[i]\n",
    "end\n",
    "\n",
    "backend = CUDABackend()\n",
    "a = KA.allocate(backend, Float32, N)\n",
    "b = KA.allocate(backend, Float32, N)\n",
    "c = similar(a)\n",
    "\n",
    "vadd_kernel = vadd(backend)\n",
    "vadd_kernel(a, b, c; ndrange=size(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcXK1M6TJjwv"
   },
   "source": [
    "#### Asynchronous operations\n",
    "\n",
    "GPU operations are asynchronous with regards to the host! They are **ordered** with respect to each other, but special care must be taken when using Julia's task based programming together with GPU programming.\n",
    "\n",
    "The JuliaGPU ecosystem **synchronizes** the GPU on access, so when you move data from and to the GPU we wait for all the kernels to finish!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zfgVGjrJjww"
   },
   "source": [
    "When benchmarking you need to synchronize the device!\n",
    "\n",
    "```julia\n",
    "@benchmark begin\n",
    "    vadd_kernel(a, b, c; ndrange=size(c))\n",
    "    KA.synchronize(backend)\n",
    "end\n",
    "```\n",
    "\n",
    "Otherwise you are only measuring the **launch** of the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LW51Tc5Jjwz"
   },
   "source": [
    "### What makes an application portable?\n",
    "\n",
    "1. Can I **run** it on a different compute architecture\n",
    "    1. Different CPU architectures\n",
    "    2. We live in a mult GPU vendor world\n",
    "2. Does it **compute** the same thing?\n",
    "    1. Can I develop on one platform and move to another later?\n",
    "3. Does it achieve the same **performance**?\n",
    "4. Can I take advantage of platform **specific** capabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hEawmex5Jjw0"
   },
   "source": [
    "#### Adapt.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsLBPf2nJjw0"
   },
   "source": [
    "[Adapt.jl](https://github.com/JuliaGPU/Adapt.jl) is a lightweight dependency that you can use to convert complex structures from CPU to GPU.\n",
    "\n",
    "```julia\n",
    "using Adapt\n",
    "adapt(CuArray, ::Adjoint{Array})::Adjoint{CuArray}\n",
    "```\n",
    "\n",
    "```julia\n",
    "struct Model{T<:Number, AT<:AbstractArray{T}}\n",
    "   data::AT\n",
    "end\n",
    "\n",
    "Adapt.adapt_structure(to, x::Model) = Model(adapt(to, x.data))\n",
    "\n",
    "cpu = Model(rand(64, 64));\n",
    "using CUDA\n",
    "\n",
    "gpu = adapt(CuArray, cpu)\n",
    "Model{Float64, CuArray{Float64, 2, CUDA.Mem.DeviceBuffer}}(...)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxXONwE_UYGF"
   },
   "source": [
    "## GPU kernel -- transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RdwzZOQ9VQWw",
    "outputId": "113e0401-d903-4a26-abc3-2966c99659c9"
   },
   "outputs": [],
   "source": [
    "const nreps = 3\n",
    "const N = 2048\n",
    "const T = Float32\n",
    "\n",
    "const TILE_DIM = 32\n",
    "const BLOCK_ROWS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O732YI3WWCnE"
   },
   "source": [
    "### Naive kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RWDdbIl1UjYC",
    "outputId": "c5f150ab-e20f-4fd7-9b09-bf900ef14a7b"
   },
   "outputs": [],
   "source": [
    "@kernel function simple_copy_kernel!(output, @Const(input))\n",
    "    I, J = @index(Global, NTuple)\n",
    "    @inbounds output[I, J] = input[I, J]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k3sMLN2lT7s2",
    "outputId": "f4478256-b35e-473a-cbb1-4ef80cceadf6"
   },
   "outputs": [],
   "source": [
    "@kernel function simple_transpose_kernel!(output, @Const(input))\n",
    "    I, J = @index(Global, NTuple)\n",
    "    @inbounds output[J, I] = input[I, J]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nbG1_3QPWFhN"
   },
   "source": [
    "### Using localmemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KEBb8iCUUg2Q",
    "outputId": "321302ec-d358-47c8-f8fa-56956073a03f"
   },
   "outputs": [],
   "source": [
    "@kernel unsafe_indices = true function lmem_copy_kernel!(\n",
    "        output, @Const(input),\n",
    "        ::Val{BANK} = Val(1),\n",
    "    ) where {BANK}\n",
    "    I, J = @index(Global, NTuple)\n",
    "    i, j = @index(Local, NTuple)\n",
    "\n",
    "    N = @uniform @groupsize()[1]\n",
    "    M = @uniform @groupsize()[2]\n",
    "\n",
    "    # +1 to avoid bank conflicts on shared memory\n",
    "    tile = @localmem eltype(output) (N + BANK, M)\n",
    "\n",
    "    @inbounds tile[i, j] = input[I, J]\n",
    "\n",
    "    @synchronize\n",
    "\n",
    "    @inbounds output[I, J] = tile[i, j]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZHI9w-ggUE3y",
    "outputId": "075333c3-d4d3-4f0c-9fc0-90f5c9466a2a"
   },
   "outputs": [],
   "source": [
    "@kernel unsafe_indices = true function lmem_transpose_kernel!(\n",
    "        output, @Const(input),\n",
    "        ::Val{BANK} = Val(1),\n",
    "    ) where {BANK}\n",
    "    gi, gj = @index(Group, NTuple)\n",
    "    i, j = @index(Local, NTuple)\n",
    "\n",
    "    N = @uniform @groupsize()[1]\n",
    "    M = @uniform @groupsize()[2]\n",
    "\n",
    "    # +1 to avoid bank conflicts on shared memory\n",
    "    tile = @localmem eltype(output) (N + BANK, M)\n",
    "\n",
    "    # Manually calculate global indexes\n",
    "    # Later on we need to pivot the group index\n",
    "    I = (gi - 1) * N + i\n",
    "    J = (gj - 1) * M + j\n",
    "\n",
    "    @inbounds tile[i, j] = input[I, J]\n",
    "\n",
    "    @synchronize\n",
    "\n",
    "    # Pivot the group index\n",
    "    I = (gj - 1) * M + i\n",
    "    J = (gi - 1) * N + j\n",
    "\n",
    "    @inbounds output[I, J] = tile[j, i]\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "peMDm9SvVtrv"
   },
   "source": [
    "### Local Memory + process multiple elements per lane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JNhPHFEqU3Si"
   },
   "outputs": [],
   "source": [
    "using KernelAbstractions.Extras: @unroll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "inJxG14QVrKe",
    "outputId": "35a2b35c-52f5-4fd9-961c-43d45745b262"
   },
   "outputs": [],
   "source": [
    "@kernel unsafe_indices=true function coalesced_copy_kernel!(\n",
    "        output, @Const(input),\n",
    "        ::Val{BANK} = Val(1),\n",
    "    ) where {BANK}\n",
    "    gi, gj = @index(Group, NTuple)\n",
    "    i, j = @index(Local, NTuple)\n",
    "\n",
    "    TILE_DIM = @uniform @groupsize()[1]\n",
    "    BLOCK_ROWS = @uniform @groupsize()[2]\n",
    "\n",
    "    # +1 to avoid bank conflicts on shared memory\n",
    "    tile = @localmem eltype(output) (TILE_DIM + BANK, TILE_DIM)\n",
    "\n",
    "    # Can't use @index(Global), because we use a smaller ndrange\n",
    "    I = (gi - 1) * TILE_DIM + i\n",
    "    J = (gj - 1) * TILE_DIM + j\n",
    "\n",
    "    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM - 1)\n",
    "        @inbounds tile[i, j + k] = input[I, J + k]\n",
    "    end\n",
    "\n",
    "    @synchronize\n",
    "\n",
    "    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM - 1)\n",
    "        @inbounds output[I, J + k] = tile[i, j + k]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XkybpxxlUS-2",
    "outputId": "fbc8dbad-31c3-4025-b945-68a46836de3d"
   },
   "outputs": [],
   "source": [
    "@kernel unsafe_indices = true function coalesced_transpose_kernel!(\n",
    "        output, @Const(input),\n",
    "        ::Val{BANK} = Val(1),\n",
    "    ) where {BANK}\n",
    "    gi, gj = @index(Group, NTuple)\n",
    "    i, j = @index(Local, NTuple)\n",
    "\n",
    "    TILE_DIM = @uniform @groupsize()[1]\n",
    "    BLOCK_ROWS = @uniform @groupsize()[2]\n",
    "\n",
    "    # +1 to avoid bank conflicts on shared memory\n",
    "    tile = @localmem eltype(output) (TILE_DIM + BANK, TILE_DIM)\n",
    "\n",
    "    # Can't use @index(Global), because we use a smaller ndrange\n",
    "    I = (gi - 1) * TILE_DIM + i\n",
    "    J = (gj - 1) * TILE_DIM + j\n",
    "\n",
    "    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM - 1)\n",
    "        @inbounds tile[i, j + k] = input[I, J + k]\n",
    "    end\n",
    "\n",
    "    @synchronize\n",
    "\n",
    "    # Transpose block offsets\n",
    "    I = (gj - 1) * TILE_DIM + i\n",
    "    J = (gi - 1) * TILE_DIM + j\n",
    "\n",
    "    @unroll for k in 0:BLOCK_ROWS:(TILE_DIM - 1)\n",
    "        @inbounds output[I, J + k] = tile[j + k, i]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq2pCjP4V6wf"
   },
   "source": [
    "### Benchmark harness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AsJ95M-OVKPv"
   },
   "outputs": [],
   "source": [
    "using NVTX, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5lLdmw2nV-WT",
    "outputId": "d67c9e94-52e9-4d52-ef37-6618ea382991"
   },
   "outputs": [],
   "source": [
    "backend = CUDABackend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z0A6CvDaVMnz",
    "outputId": "092aebc1-a8fc-4397-c308-25d2335f61d7"
   },
   "outputs": [],
   "source": [
    "CUDA.@profile for block_dims in ((TILE_DIM, TILE_DIM), (TILE_DIM * TILE_DIM, 1), (1, TILE_DIM * TILE_DIM))\n",
    "    for (name, kernel) in (\n",
    "            (\"copy\", simple_copy_kernel!(backend, block_dims)),\n",
    "            (\"transpose\", simple_transpose_kernel!(backend, block_dims)),\n",
    "        )\n",
    "        NVTX.@range \"Simple $name $block_dims\" let\n",
    "            input = rand!(allocate(backend, T, N, N))\n",
    "            output = similar(input)\n",
    "\n",
    "            # compile kernel\n",
    "            kernel(output, input, ndrange = size(output))\n",
    "            for rep in 1:nreps\n",
    "                kernel(output, input, ndrange = size(output))\n",
    "            end\n",
    "            KernelAbstractions.synchronize(backend)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FwL2dy7XVd4Z",
    "outputId": "e5b82069-5491-43bd-b6d4-76bcc7c0db82"
   },
   "outputs": [],
   "source": [
    "# Benchmark localmem\n",
    "CUDA.@profile for (name, kernel) in (\n",
    "        (\"copy\", lmem_copy_kernel!(backend, (TILE_DIM, TILE_DIM))),\n",
    "        (\"transpose\", lmem_transpose_kernel!(backend, (TILE_DIM, TILE_DIM))),\n",
    "    )\n",
    "    for bank in (true, false)\n",
    "        NVTX.@range \"Localmem $name ($TILE_DIM, $TILE_DIM) bank=$bank\" let\n",
    "            input = rand!(allocate(backend, T, N, N))\n",
    "            output = similar(input)\n",
    "\n",
    "            # compile kernel\n",
    "            kernel(output, input, Val(Int(bank)), ndrange = size(output))\n",
    "            for rep in 1:nreps\n",
    "                kernel(output, input, Val(Int(bank)), ndrange = size(output))\n",
    "            end\n",
    "            KernelAbstractions.synchronize(backend)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qXxdCZBVVfj1",
    "outputId": "001f2805-91ab-4c25-eadc-a87ed3cf07dc"
   },
   "outputs": [],
   "source": [
    "# Benchmark localmem + multiple elements per lane\n",
    "CUDA.@profile for (name, kernel) in (\n",
    "        (\"copy\", coalesced_copy_kernel!(backend, (TILE_DIM, BLOCK_ROWS))),\n",
    "        (\"transpose\", coalesced_transpose_kernel!(backend, (TILE_DIM, BLOCK_ROWS))),\n",
    "    )\n",
    "    for bank in (true, false)\n",
    "        NVTX.@range \"Localmem + multiple elements $name ($TILE_DIM, $BLOCK_ROWS) bank=$bank\" let\n",
    "            input = rand!(allocate(backend, T, N, N))\n",
    "            output = similar(input)\n",
    "\n",
    "            # We want a number of blocks equivalent to (TILE_DIM, TILE_DIM)\n",
    "            # but our blocks are (TILE_DIM, BLOCK_ROWS) so we need to remove\n",
    "            # a factor from the size of the array otherwise we get to many blocks\n",
    "            block_factor = div(TILE_DIM, BLOCK_ROWS)\n",
    "            ndrange = (N, div(N, block_factor))\n",
    "\n",
    "            # compile kernel\n",
    "            kernel(output, input, Val(Int(bank)), ndrange = ndrange)\n",
    "            for rep in 1:nreps\n",
    "                kernel(output, input, Val(Int(bank)), ndrange = ndrange)\n",
    "            end\n",
    "            KernelAbstractions.synchronize(backend)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNekriU2bSRB"
   },
   "source": [
    "## Atomic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yV55CsqMW1e0",
    "outputId": "df8dc127-bd63-4cb2-87f2-857b005387a3"
   },
   "outputs": [],
   "source": [
    "@kernel function racy_kernel!(out, arr)\n",
    "\ti, j = @index(Global, NTuple)\n",
    "\tfor k in 1:size(out, 1)\n",
    "\t\tout[k, i] += arr[i, j]\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y9j2kFZ1ceBj"
   },
   "outputs": [],
   "source": [
    "using ImageShow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "1wLZB_bHbYiJ",
    "outputId": "40f3f1a8-6cb8-4b57-d29c-af7332a6a222"
   },
   "outputs": [],
   "source": [
    "img = zeros(Float32, (50, 50));\n",
    "img[10:20, 10:20] .= 1;\n",
    "img[35:45, 35:45] .= 2;\n",
    "simshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "Y8t0Q2QpbdkM",
    "outputId": "15354de3-18d7-4e7d-d04a-266968b297b6"
   },
   "outputs": [],
   "source": [
    "out = KernelAbstractions.zeros(backend, Float32, size(img));\n",
    "racy_kernel!(backend)(out, adapt(backend, img), ndrange=size(img))\n",
    "simshow(Array(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V02D9krWbvAN"
   },
   "outputs": [],
   "source": [
    "using Atomix: @atomic, @atomicswap, @atomicreplace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jD0eol-2cx41",
    "outputId": "d77fe6bc-4778-4c47-82a5-51f57232edb7"
   },
   "outputs": [],
   "source": [
    "@kernel function nonracy_kernel!(out, arr)\n",
    "\ti, j = @index(Global, NTuple)\n",
    "\tfor k in 1:size(out, 1)\n",
    "\t\t@atomic out[k, i] += arr[i, j]\n",
    "\tend\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "Sa-Gzr7tc61f",
    "outputId": "4adde915-9d32-44d2-829a-dd14162e1540"
   },
   "outputs": [],
   "source": [
    "out = KernelAbstractions.zeros(backend, Float32, size(img));\n",
    "nonracy_kernel!(backend)(out, adapt(backend, img), ndrange=size(img))\n",
    "simshow(Array(out))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RoCwrdAKd1Ut"
   },
   "source": [
    "## Matrix multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jJ7ErQTfd4ZO",
    "outputId": "263e9c3b-580a-4edc-a661-cce2c4ca3fab"
   },
   "outputs": [],
   "source": [
    "@kernel function naive_matmul_kernel!(output, a, b)\n",
    "    i, j = @index(Global, NTuple)\n",
    "\n",
    "    # creating a temporary sum variable for matrix multiplication\n",
    "    tmp_sum = zero(eltype(output))\n",
    "    for k in 1:size(a)[2]\n",
    "        tmp_sum += a[i, k] * b[k, j]\n",
    "    end\n",
    "\n",
    "    output[i, j] = tmp_sum\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-miQrox1ebjt"
   },
   "outputs": [],
   "source": [
    "# Creating a wrapper kernel for launching with error checks\n",
    "function naive_matmul!(output, a, b)\n",
    "    if size(a)[2] != size(b)[1]\n",
    "        println(\"Matrix size mismatch!\")\n",
    "        return nothing\n",
    "    end\n",
    "    backend = KernelAbstractions.get_backend(a)\n",
    "    kernel! = naive_matmul_kernel!(backend)\n",
    "    kernel!(output, a, b, ndrange = size(output))\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc4MplI9d89w"
   },
   "outputs": [],
   "source": [
    "let\n",
    "  a = rand!(allocate(backend, Float32, 256, 123))\n",
    "  b = rand!(allocate(backend, Float32, 123, 45))\n",
    "  output = KernelAbstractions.zeros(backend, Float32, 256, 45)\n",
    "\n",
    "  naive_matmul!(output, a, b)\n",
    "\n",
    "  @assert isapprox(output, a * b)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mN1LbGM7eGZL"
   },
   "outputs": [],
   "source": [
    "@kernel unsafe_indices = true function coalesced_matmul_kernel!(\n",
    "        output, @Const(A), @Const(B),\n",
    "        ::Val{BANK} = Val(1),\n",
    "    ) where {BANK}\n",
    "    gi, gj = @index(Group, NTuple)\n",
    "    i, j = @index(Local, NTuple)\n",
    "\n",
    "    TILE_DIM = @uniform @groupsize()[1]\n",
    "\n",
    "    # +1 to avoid bank conflicts on shared memory\n",
    "    tile1 = @localmem eltype(output) (TILE_DIM + BANK, TILE_DIM)\n",
    "    tile2 = @localmem eltype(output) (TILE_DIM + BANK, TILE_DIM)\n",
    "\n",
    "    # private variable for tile output\n",
    "    outval = @private eltype(output) 1\n",
    "    @inbounds outval[1] = -zero(eltype(output))\n",
    "\n",
    "    @uniform N = size(output, 1)\n",
    "    @uniform M = size(output, 2)\n",
    "    @uniform R = size(A, 2)\n",
    "    # number of tiles depends on inner dimension\n",
    "    @uniform NUM_TILES = div(R + TILE_DIM - 1, TILE_DIM)\n",
    "\n",
    "    # loop over all tiles needed for this calculation\n",
    "    for t in 0:(NUM_TILES - 1)\n",
    "        # Can't use @index(Global), because we use a smaller ndrange\n",
    "        I = (gi - 1) * TILE_DIM + i\n",
    "        J = (gj - 1) * TILE_DIM + j\n",
    "\n",
    "        # load inputs into tiles, with bounds checking for non-square matrices\n",
    "        if I <= N && t * TILE_DIM + j <= R\n",
    "            @inbounds tile1[i, j] = A[I, t * TILE_DIM + j]\n",
    "        else\n",
    "            @inbounds tile1[i, j] = 0.0\n",
    "        end\n",
    "        if t * TILE_DIM + i <= R && J <= M\n",
    "            @inbounds tile2[i, j] = B[t * TILE_DIM + i, J]\n",
    "        else\n",
    "            @inbounds tile2[i, j] = 0.0\n",
    "        end\n",
    "\n",
    "        # wait for all tiles to be loaded\n",
    "        @synchronize\n",
    "\n",
    "        # get global values again\n",
    "        I = (gi - 1) * TILE_DIM + i\n",
    "        J = (gj - 1) * TILE_DIM + j\n",
    "\n",
    "        # calculate value of spot in output, use temporary value to allow for vectorization\n",
    "        out = zero(eltype(output))\n",
    "        @simd for k in 1:TILE_DIM\n",
    "            @inbounds out += tile1[i, k] * tile2[k, j]\n",
    "        end\n",
    "        outval[1] += out\n",
    "\n",
    "        @synchronize\n",
    "    end\n",
    "\n",
    "    # get global indices again\n",
    "    I = (gi - 1) * TILE_DIM + i\n",
    "    J = (gj - 1) * TILE_DIM + j\n",
    "\n",
    "    # save if inbounds\n",
    "    if I <= N && J <= M\n",
    "        @inbounds output[I, J] = outval[1]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dE4LNL5Zecj3",
    "outputId": "89ad6250-24b3-4d13-fde9-95731853b1de"
   },
   "outputs": [],
   "source": [
    "# Creating a wrapper kernel for launching with error checks\n",
    "function coalesced_matmul!(output, a, b)\n",
    "    if size(a)[2] != size(b)[1]\n",
    "        println(\"Matrix size mismatch!\")\n",
    "        return nothing\n",
    "    end\n",
    "    backend = KernelAbstractions.get_backend(a)\n",
    "    kernel! = coalesced_matmul_kernel!(backend, (TILE_DIM, TILE_DIM))\n",
    "    kernel!(output, a, b, ndrange = size(output))\n",
    "    return\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jM9J-KMnetKN"
   },
   "outputs": [],
   "source": [
    "let\n",
    "  a = rand!(allocate(backend, Float32, 256, 123))\n",
    "  b = rand!(allocate(backend, Float32, 123, 45))\n",
    "  output = KernelAbstractions.zeros(backend, Float32, 256, 45)\n",
    "\n",
    "  coalesced_matmul!(output, a, b)\n",
    "\n",
    "  @assert isapprox(output, a * b)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3ZPZNcmfv3b"
   },
   "outputs": [],
   "source": [
    "import LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iw9Uuzn0hM0I"
   },
   "source": [
    "### Exercise\n",
    "- Vary N, R, M\n",
    "- Vary T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bOwz86oLfYjs",
    "outputId": "fa4beb7d-9c82-4411-8fe9-5e12e37f6e8f"
   },
   "outputs": [],
   "source": [
    "let\n",
    "    N = 1024\n",
    "    R = 512\n",
    "    M = 2048\n",
    "    T = Float64\n",
    "    A = rand!(allocate(backend, T, N, R))\n",
    "    B = rand!(allocate(backend, T, R, M))\n",
    "    output_naive = KernelAbstractions.zeros(backend, T, N, M)\n",
    "    output_coalesced = KernelAbstractions.zeros(backend, T, N, M)\n",
    "    output_mul = KernelAbstractions.zeros(backend, T, N, M)\n",
    "\n",
    "\n",
    "    CUDA.@profile for _ in 1:nreps\n",
    "      NVTX.@range \"Naive Matmul\" begin\n",
    "          naive_matmul!(output_naive, A, B)\n",
    "          KernelAbstractions.synchronize(backend)\n",
    "      end\n",
    "\n",
    "      NVTX.@range \"Coalesced Matmul\" begin\n",
    "          coalesced_matmul!(output_coalesced, A, B)\n",
    "          KernelAbstractions.synchronize(backend)\n",
    "      end\n",
    "\n",
    "      NVTX.@range \"LinearAlgebra.mul!\" begin\n",
    "          LinearAlgebra.mul!(output_mul, A, B)\n",
    "          KernelAbstractions.synchronize(backend)\n",
    "      end\n",
    "    end\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Julia",
   "name": "julia"
  },
  "language_info": {
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
