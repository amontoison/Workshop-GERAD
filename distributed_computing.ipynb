{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing and GPU programming with Julia \n",
    "## Part II: Multi-processing and distributed computing\n",
    "Alexis Montoison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using Distributed\n",
    "using MPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Graphics/multithreading_multiprocessing.png\" width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An implementation of distributed memory parallel computing is provided by module `Distributed` as part of the standard library shipped with Julia.\n",
    "\n",
    "Most modern computers possess more than one CPU, and several computers can be combined together in a cluster. Harnessing the power of these multiple CPUs allows many computations to be completed more quickly. There are two major factors that influence performance: the speed of the CPUs themselves, and the speed of their access to memory. In a cluster, it's fairly obvious that a given CPU will have fastest access to the RAM within the same computer (node). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Graphics/distributed_computing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia provides a multiprocessing environment based on message passing to allow programs to run on multiple processes in separate memory domains at once.\n",
    "\n",
    "Julia’s main implementation of message passing for distributed-memory systems is contained in the Distributed module. Its approach is different from other frameworks like MPI in that communication is generally “one-sided”, meaning that the programmer needs to explicitly manage only one process in a two-process operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Graphics/distributedjl_model.svg\" width=850>\n",
    "\n",
    "**Master-worker paradigm**:\n",
    "* One master process coordinates a set of worker processes (that eventually perform computations).\n",
    "* Programmer only controls the master directly. The workers are \"instructed\" (**one-sided** communication).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia can be started with a given number of `p` local workers with:\n",
    "```julia\n",
    "julia -p 4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Distributed module is automatically loaded if the `-p` flag is used. But we can also dynamically add processes in a running Julia session:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Distributed\n",
    "\n",
    "println(nprocs())\n",
    "addprocs(4)         # add 4 workers\n",
    "println(nprocs())   # total number of processes\n",
    "println(nworkers()) # only worker processes\n",
    "# rmprocs(workers())  # remove worker processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note what happens here: there is one master process which can create additional worker processes, and as we shall see, it can also distribute work to these workers.\n",
    "For running on a cluster, we instead need to provide the `--machine-file` option and the name of a file containing a list of machines that are accessible via password-less `ssh`. Support for running on clusters with various schedulers (including SLURM) can be found in the [ClusterManagers.jl](https://github.com/JuliaParallel/ClusterManagers.jl) package.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync pmap(i -> println(\"I'm worker $(myid()), working on i=$i\"), 1:10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@sync @distributed for i in 1:10\n",
    "  println(\"I'm worker $(myid()), working on i=$i\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pmap` should be preferred when each operation is not very fast.\n",
    "`@distributed` for is better when each operation is very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [i for i=1:10]\n",
    "@sync @distributed for i in 1:10\n",
    "  println(\"working on i=$i\")\n",
    "  a[i] = i^2\n",
    "end\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nothing changed in a!! a is made available to each worker, but it’s basically for “reading” because a whole copy of a is sent to the memory space of each worker. Each worker has its own memory domain, for safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printsquare(i) = println(\"working on i=$i: its square it $(i^2)\")\n",
    "@sync @distributed for i in 1:10\n",
    "  printsquare(i) # error\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this to work, we need to export functions and packages `@everywhere`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere printsquare(i) = println(\"working on i=$i: its square it $(i^2)\")\n",
    "@sync @distributed for i in 1:10\n",
    "  printsquare(i)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: 1 worker ≠ 1 CPU core or thread: if we ask for 50 processors and our machine only has 4, we will see that we have 50 workers, but several workers will be sharing the same CPU (but different memory domains). It will slow us down compared to asking for 4\n",
    "workers only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each process has a unique identifier accessible via the `myid()` function (master has `myid() == 1`).\n",
    "The `@spawn` and `@spawnat` macros can be used to transfer work to a process, and then return the `Future` result to the master process using the `fetch` function. `@spawn` selects the process automatically while `@spawnat` lets you choose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute myid() and rand() on process 2\n",
    "r = @spawnat 2 (myid(), rand())\n",
    "\n",
    "# fetch the result\n",
    "fetch(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One use case could be to manually distribute expensive function calls between processes, but there are higher-level and simpler constructs than @spawn / @spawnat:\n",
    "- the `@distributed` macro for for loops. Can be used with a reduction operator to gather work performed by the independent tasks.\n",
    "- the pmap function which maps an array or range to a given function.\n",
    "\n",
    "To illustrate the difference between these approaches we revisit the `sum_sqrt` function from the Multithreading notebook. To use `pmap` we need to modify our function to accept a range so we will use this modified version. Note that to make any function available to all processes it needs to be decorated with the @everywhere macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@everywhere function sqrt_sum_range(A, r)\n",
    "    s = zero(eltype(A))\n",
    "    for i in r\n",
    "        @inbounds s += sqrt(A[i])\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(100_000)\n",
    "batch = Int(length(A) / 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distributed (+)\n",
    "@distributed (+) for r in [(1:batch) .+ offset for offset in 0:batch:length(A)-1]\n",
    "    sqrt_sum_range(A, r)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pmap\n",
    "sum(pmap(r -> sqrt_sum_range(A, r), [(1:batch) .+ offset for offset in 0:batch:length(A)-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @spawnat\n",
    "futures = Array{Future}(undef, nworkers())\n",
    "\n",
    "@time begin\n",
    "    for (i, id) in enumerate(workers())\n",
    "        batch = floor(Int, length(A) / nworkers())\n",
    "        remainder = length(A) % nworkers()\n",
    "        if (i-1) < remainder\n",
    "            start = 1 + (i - 1) * (batch + 1)\n",
    "            stop = start + batch\n",
    "        else\n",
    "            start = 1 + (i - 1) * batch + remainder\n",
    "            stop = start + batch - 1\n",
    "        end\n",
    "        futures[i] = @spawnat myid() sqrt_sum_range(A, start:stop)\n",
    "    end\n",
    "    p = sum(fetch.(futures))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `@spawnat` version looks cumbersome for this case particular case as the algorithm required the explicit partitioning of the array which is common in MPI, for instance. The `@distributed` (+) parallel for loop and the `pmap` mapping are much simpler, but which one is preferable for a given use case?\n",
    "- `@distributed` is appropriate for reductions. It does not load-balance and simply divides the work evenly between processes. It is best in cases where each loop iteration is cheap.\n",
    "- `pmap` can handle reductions as well as other algorithms. It performs load-balancing and since dynamic scheduling introduces some overhead it’s best to use pmap for computationally heavy tasks.\n",
    "- In the case of `@spawnat`, because the futures are not immediately using CPU resources, it opens the possibility of using asynchronous and uneven workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with multithreading, multiprocessing with Distributed comes with an overhead because of sending messages and moving data between processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, it should be emphasized that a common use case of pmap involves heavy computations inside functions defined in imported packages. For example, computing the singular value decomposition of many matrices:\n",
    "@everywhere using LinearAlgebra\n",
    "x=[rand(100,100) for i in 1:10]\n",
    "@btime map(LinearAlgebra.svd, x);\n",
    "@btime pmap(LinearAlgebra.svd, x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DistributedArrays**: Another way to approach parallelization over multiple machines is through [DistributedArrays.jl](https://github.com/JuliaParallel/DistributedArrays.jl) package. A `DistributedArrays` is distributed across a set of workers. Each worker can read and write from its local portion of the array and each worker has read-only access to the portions of the array held by other workers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "- `@distributed` is good for reductions and fast inner loops with limited data transfer.\n",
    "- `pmap` is good for expensive inner loops that return a value.\n",
    "- `SharedArrays` can be an easier drop-in replacement for threading-like behaviors on a single machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPI\n",
    "[MPI.jl](https://github.com/JuliaParallel/MPI.jl) is a Julia interface to the Message Passing Interface, which has been the standard workhorse of parallel computing for decades. Like `Distributed`, MPI belongs to the distributed-memory paradigm.\n",
    "\n",
    "The idea behind MPI is that:\n",
    "- Tasks have a rank and are numbered 0, 1, 2, 3, …\n",
    "- Each task manages its own memory\n",
    "- Each task can run multiple threads\n",
    "- Tasks communicate and share data by sending messages.\n",
    "- Many higher-level functions exist to distribute information to other tasks and gather information from other tasks.\n",
    "- All tasks typically run the entire code and we have to be careful to avoid that all tasks do the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Graphics/mpi_model.svg\" width=850>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MPI.jl provides Julia bindings for the Message Passing Interface (MPI) standard. This is how a hello world MPI program looks like in Julia:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fundamental MPI functions\n",
    "\n",
    "* `MPI.Init()` and `MPI.Finalize()` (the latter isn't necessary in Julia)\n",
    "\n",
    "* `MPI.COMM_WORLD`: default *communicator*, includes all MPI ranks\n",
    "\n",
    "* `MPI.Comm_rank(comm)`: unique rank of the process calling this function (**MPI rank ids start at 0!**)\n",
    "\n",
    "* `MPI.Comm_size(comm)`: total number of ranks in the given communicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MPI\n",
    "MPI.Init()\n",
    "comm = MPI.COMM_WORLD  # MPI.COMM_WORLD is the communicator - a group of processes that can talk to each other\n",
    "rank = MPI.Comm_rank(comm)  # Comm_rank returns the individual rank (0, 1, 2, …) for each task that calls it\n",
    "size = MPI.Comm_size(comm)  # Comm_size returns the total number of ranks.\n",
    "println(\"Hello from process $(rank) out of $(size)\")\n",
    "MPI.Barrier(comm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MPI.jl package contains a lot of functionality, but in principle one can get away with only point-to-point communication (`MPI.send()` and `MPI.recv()`). However, collective communication can sometimes require less effort. In any case, it is good to have a mental model of different communication patterns in MPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MPI\n",
    "MPI.Init()\n",
    "\n",
    "comm = MPI.COMM_WORLD\n",
    "rank = MPI.Comm_rank(comm)\n",
    "size = MPI.Comm_size(comm)\n",
    "\n",
    "if rank != 0\n",
    "    # All ranks other than 0 should send a message\n",
    "    local message = \"Hello World, I'm rank $rank\"\n",
    "    MPI.send(message, comm, dest=0, tag=0)\n",
    "else\n",
    "    # Rank 0 will receive each message and print them\n",
    "    for sender in 1:(size-1)\n",
    "        message = MPI.recv(comm, source=sender, tag=0)\n",
    "        println(message)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary**:\n",
    "- MPI is a standard work-horse of parallel computing.\n",
    "- All communication is handled explicitly - not behind the scenes as in `Distributed`.\n",
    "- Programming with MPI requires a different mental model since each parallel rank is executing the same program and the programmer needs to distribute the work by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Graphics/meme_distributed_computing.png' width='400'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- https://enccs.github.io/Julia-for-HPC/distributed\n",
    "- https://enccs.github.io/Julia-for-HPC/MPI\n",
    "- https://enccs.github.io/Julia-for-HPC/cluster\n",
    "- https://github.com/hlrs-tasc/julia-on-hpc-systems\n",
    "- https://docs.julialang.org/en/v1/manual/distributed-computing\n",
    "- https://hiteshmishra708.medium.com/multiprocessing-in-python-c6735fa70f3f\n",
    "- https://sites.google.com/site/cis115textbook/distributed-computing\n",
    "- http://cecileane.github.io/computingtools/pages/notes1209.html\n",
    "- https://github.com/JuliaParallel/MPI.jl\n",
    "- https://memegenerator.net/instance/68378447"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
