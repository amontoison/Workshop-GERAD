{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parallel computing and GPU programming with Julia \n",
    "## Part III: GPU programming\n",
    "Alexis Montoison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "using CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Julia has first-class support for GPU programming through the following packages that target GPUs from major vendors:\n",
    "- [CUDA.jl](https://github.com/JuliaGPU/CUDA.jl) for NVIDIA GPUs\n",
    "- [AMDGPU.jl](https://github.com/JuliaGPU/AMDGPU.jl) for AMD GPUs\n",
    "- [oneAPI.jl](https://github.com/JuliaGPU/oneAPI.jl) for Intel GPUs\n",
    "- [Metal.jl](https://github.com/JuliaGPU/Metal.jl) for Apple M-series GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA.jl is the most mature and we will use it for the workshop.\n",
    "AMDGPU.jl is somewhat behind but still ready for general use, while oneAPI.jl and Metal.jl are functional but might contain bugs, miss some features and provide suboptimal performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between a CPU and a GPU? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Graphics/cpu_vs_gpu.png' width='700'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='./Graphics/meme_gpu.jpg' width='300'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some key aspects of GPUs that need to be kept in mind:\n",
    "- The large number of compute elements on a GPU (in the thousands) can enable extreme scaling for data parallel tasks.\n",
    "- GPUs have their own memory. This means that data needs to be transfered to and from the GPU during the execution of a program.\n",
    "- Cores in a GPU are arranged into a particular structure. At the highest level they are divided into “streaming multiprocessors” (SMs). Some of these details are important when writing own GPU kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Graphics/gpu.png\" width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./Graphics/gpu_topology.svg\" width=500px>\n",
    "\n",
    "* **host**: CPU + system memory (host memory)\n",
    "* **device**: GPU with its memory (device memory)\n",
    "* **SM**: Streaming Multiprocessor\n",
    "\n",
    "Communication:\n",
    "* Host-device bandwidth: **31.5 GB/s**\n",
    "* GPU global memory bandwidth: **1555 GB/s**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU programming with Julia can be as simple as using a different array type instead of regular `Base.Array` arrays:\n",
    "- `CuArray` from CUDA.jl for NVIDIA GPUs\n",
    "- `ROCArray` from AMDGPU.jl for AMD GPUs\n",
    "- `oneArray` from oneAPI.jl for Intel GPUs\n",
    "- `MtlArray` from Metal.jl for Apple GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These array types are subtypes of `GPUArrays` from [GPUArrays.jl](https://github.com/JuliaGPU/GPUArrays.jl) and closely resemble `Base.Array` which enables us to write generic code which works on both CPU and GPU arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    A_d = CuArray([1,2,3,4])\n",
    "    A_d .+= 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do the same operation with other subtypes of `GPUArrays`:\n",
    "```julia\n",
    "if AMDGPU.functional()\n",
    "    A_d = ROCArray([1,2,3,4])\n",
    "    A_d .+= 1\n",
    "end\n",
    "\n",
    "if oneAPI.functional()\n",
    "    A_d = oneArray([1,2,3,4])\n",
    "    A_d .+= 1\n",
    "end\n",
    "\n",
    "A_d = MtlArray([1,2,3,4])\n",
    "A_d .+= 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving an array back from the GPU to the CPU is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    A = Array(A_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"./Graphics/cpu_gpu_transfer.svg\" width=180px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the overhead of copying data to the GPU makes such simple calculations very slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s have a look at a more realistic example: matrix multiplication.\n",
    "We create two random arrays, one on the CPU and one on the GPU, and compare the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    A = rand(2^12, 2^12)\n",
    "    A_d = CuArray(A)\n",
    "\n",
    "    @btime $A * $A\n",
    "    CUDA.@time A_d * A_d\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    A = rand(Float32, 2^12, 2^12)\n",
    "    A_d = CuArray(A)\n",
    "    @btime $A * $A\n",
    "    CUDA.@time A_d * A_d\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPUs normally perform significantly better for 32-bit floats. Some GPUs doesn't support 64-bit floats!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many array operations in Julia are implemented using loops, processing one element at a time. Doing so with GPU arrays is very ineffective, as the loop won't actually execute on the GPU, but transfer one element at a time and process it on the CPU. As this wrecks performance, you will be warned when performing this kind of iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    A_d[1] = 3.0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scalar indexing is only allowed in an interactive session, e.g. the REPL, because it is convenient when porting CPU code to the GPU. If you want to disallow scalar indexing, e.g. to verify that your application executes correctly on the GPU, call the allowscalar function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    CUDA.allowscalar(false)\n",
    "    A_d[1] = 3.0\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a non-interactive session, e.g. when running code from a script or application, scalar indexing is disallowed by default. There is no global toggle to allow scalar indexing; if you really need it, you can mark expressions using allowscalar with do-block syntax or `@allowscalar` macro:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    CUDA.allowscalar(false)\n",
    "\n",
    "    CUDA.allowscalar() do\n",
    "        A_d[1] += 1\n",
    "    end\n",
    "\n",
    "    CUDA.@allowscalar A_d[1] += 1\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nvidia provides CUDA toolkit, a collection of libraries that contain precompiled kernels for common operations like matrix multiplication ([cuBLAS](https://docs.nvidia.com/cuda/cublas/)), fast Fourier transforms ([cuFFT](https://docs.nvidia.com/cuda/cufft/)), linear solvers ([cuSOLVER](https://docs.nvidia.com/cuda/cusolver/)), sparse linear algebra ([CUSPARSE](https://docs.nvidia.com/cuda/cusparse/)), etc.\n",
    "These kernels are wrapped in CUDA.jl and can be used directly with CuArrays."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recommended way to use CUDA.jl is to let it automatically download an appropriate CUDA toolkit. CUDA.jl will check your driver's capabilities, which versions of CUDA are available for your platform, and automatically download an appropriate artifact containing all the libraries that CUDA.jl supports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```julia\n",
    "CUDA.set_runtime_version!( v\"11.8\" )\n",
    "```\n",
    "To use a local installation, you can invoke the same API but set the version to `\"local\"`:\n",
    "```julia\n",
    "CUDA.set_runtime_version!( local_toolkit=true )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    CUDA.versioninfo()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do a guided tour of what is inside CUDA.jl!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    using CUDA.CUBLAS\n",
    "    using CUDA.CUFFT\n",
    "    using CUDA.CUSOLVER\n",
    "    using CUDA.CUSPARSE\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A powerful way to program GPUs with arrays is through Julia’s higher-order array abstractions.\n",
    "The simple element-wise addition we saw above, `a .+= 1`, is an example of this, but more general constructs can be created with `broadcast`, `map`, `reduce`, `accumulate` etc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    broadcast(-, A_d, 1)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    map(x -> x+1, A_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    reduce(+, A_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    accumulate(+, A_d)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the high-level GPU array functionality made it easy to perform this computation on the GPU. However, we didn't learn about what's going on under the hood, and that's the main goal of this tutorial. It's time to write our own kernels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vadd!(C, A, B)\n",
    "    for i in 1:length(A)\n",
    "        @inbounds C[i] = A[i] + B[i]\n",
    "    end\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ones(10)\n",
    "B = ones(10)\n",
    "C = similar(B)\n",
    "vadd!(C, A, B)\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    # We can already run this on the GPU with the @cuda macro,\n",
    "    # which will compile vadd!() into a GPU kernel and launch it\n",
    "    A_d = CuArray(A)\n",
    "    B_d = CuArray(B)\n",
    "    C_d = similar(B_d)\n",
    "    @cuda vadd!(C_d, A_d, B_d)\n",
    "    C_d\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The macros for the other GPU backends are `@roc`, `@oneapi` and `@metal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance are just terrible because each thread on the GPU would be performing the same loop! So we have to remove the loop over all elements and instead use the special `threadIdx` and `blockDim` functions, analogous respectively to `threadid` and `nthreads` for multithreading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can split work between the GPU threads by using a special function which returns the index of the GPU thread which executes it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GPU kernel**: a function that will be executed by all *GPU threads* in parallel.\n",
    "    \n",
    "Based on the index of a thread we can make them operate on different pieces of give n data.\n",
    "\n",
    "(It might be helpful to think of the GPU kernel as being the body of a loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vadd2!(C, A, B)\n",
    "    index = threadIdx().x   # linear indexing, so only use `x`\n",
    "    @inbounds C[index] = A[index] + B[index]\n",
    "    return nothing\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    N = 2^8\n",
    "    A = 2 * CUDA.ones(N)\n",
    "    B = 3 * CUDA.ones(N)\n",
    "    C = similar(B)\n",
    "\n",
    "    nthreads = N\n",
    "    @cuda threads=nthreads vadd2!(C, A, B)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    all(Array(C) .== 5.0)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The syntax is similar for the other GPU backends!\n",
    "```julia\n",
    "groupsize = length(A)\n",
    "@roc groupsize=groupsize vadd!(C, A, B)\n",
    "\n",
    "items = length(A)\n",
    "@oneapi items=items vadd!(C, A, B)\n",
    "\n",
    "nthreads = length(A)\n",
    "@metal threads=nthreads vadd!(C, A, B)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do even better, we need to parallelize more. GPUs have a limited number of threads they can run on a single streaming multiprocessor (SM), but they also have multiple SMs. To take advantage of them all, we need to run a kernel with multiple blocks. We'll divide up the work like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![gpu_threads_block](./Graphics/gpu_threads_block.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conceptual mapping:\n",
    "\n",
    "* **Grid** of blocks → entire GPU\n",
    "* **Blocks** of threads → SMs\n",
    "* **Threads** → CUDA cores\n",
    "\n",
    "**Note**: up to three dimensions, $(x, y, z)$, can be used to organize the thread blocks and threads in each block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This diagram was borrowed from a description of the NVIDIA C/C++ library; in Julia, threads and blocks begin numbering with 1 instead of 0. In this diagram, the 4096 blocks of 256 threads (making 1048576 = 2^20 threads) ensures that each thread increments just a single entry; however, to ensure that arrays of arbitrary size can be handled, let's still use a loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function vadd3!(C, A, B)\n",
    "    index = threadIdx().x + (blockIdx().x - 1) * blockDim().x\n",
    "    stride = gridDim().x * blockDim().x\n",
    "    for i = index:stride:length(B)\n",
    "        @inbounds C[index] = A[index] + B[index]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    nthreads = CUDA.attribute(device(), CUDA.DEVICE_ATTRIBUTE_MAX_THREADS_PER_BLOCK)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The maximum number of allowed threads to launch depends on your GPU!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    N = 2^14\n",
    "    A = 2 * CUDA.ones(N)\n",
    "    B = 3 * CUDA.ones(N)\n",
    "    C = similar(B)\n",
    "\n",
    "    # smallest integer larger than or equal to N / nthreads\n",
    "    numblocks = ceil(Int, N/nthreads)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CUDA.functional()\n",
    "    @cuda threads=nthreads blocks=numblocks vadd3!(C, A, B)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " all(Array(C) .== 5.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA.jl supports indexing in up to 3 dimensions (x, y and z, e.g. `threadIdx().z`). This is convenient for multidimensional data where thread blocks can be organised into 1D, 2D or 3D arrays of threads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To automatically select an appropriate number of threads, it is recommended to use the launch configuration API. This API takes a compiled (but not launched) kernel, returns a tuple with an upper bound on the number of threads, and the minimum number of blocks that are required to fully saturate the GPU:\n",
    "\n",
    "To optimize the number of threads, we can first create the kernel without launching it, query it for the number of threads supported, and then launch the compiled kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile kernel\n",
    "kernel = @cuda launch=false vadd3!(C, A, B)\n",
    "\n",
    "# extract configuration via occupancy API\n",
    "config = launch_configuration(kernel.fun)\n",
    "\n",
    "# number of threads should not exceed size of array\n",
    "threads = min(length(A), config.threads)\n",
    "\n",
    "# smallest integer larger than or equal to length(A)/threads\n",
    "blocks = cld(length(A), threads)\n",
    "\n",
    "# launch kernel with specific configuration\n",
    "kernel(C, A, B; threads, blocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Debugging**: Many things can go wrong with GPU kernel programming and unfortunately error messages are sometimes not very useful because of how the GPU compiler works.\n",
    "\n",
    "Conventional print-debugging is often a reasonably effective way to debug GPU code. CUDA.jl provides macros that facilitate this:\n",
    "- `@cushow` (like @show): visualize an expression and its result, and return that value.\n",
    "- `@cuprintln` (like println): to print text and values.\n",
    "- `@cuaassert` (like @assert) can also be useful to find issues and abort execution.\n",
    "\n",
    "GPU code introspection macros also exist, like `@device_code_warntype`, to track down type instabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function gpu_add_print!(y, x)\n",
    "    index = threadIdx().x    # this example only requires linear indexing, so just use `x`\n",
    "    stride = blockDim().x\n",
    "    @cuprintln(\"thread $index, block $stride\")\n",
    "    for i = index:stride:length(y)\n",
    "        @inbounds y[i] += x[i]\n",
    "    end\n",
    "    return nothing\n",
    "end\n",
    "\n",
    "if CUDA.functional()\n",
    "    x_d = CUDA.rand(10)\n",
    "    y_d = CUDA.rand(10)\n",
    "    @cuda threads=10 gpu_add_print!(y_d, x_d)\n",
    "    synchronize()\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: Keep in mind that the high-level functionality of CUDA often means that you don't need to worry about writing kernels at such a low level. However, there are many cases where computations can be optimized using clever low-level manipulations. The kernels implemented in Julia give you all the flexibility and performance a GPU has to offer, within a familiar language.\n",
    "\n",
    "A typical approach for porting or developing an application for the GPU is as follows:\n",
    "- develop an application using generic array functionality, and test it on the CPU with the `Array` type;\n",
    "- port your application to the GPU by switching to the `CuArray` type;\n",
    "- disallow the CPU fallback (\"scalar indexing\") to find operations that are not implemented for or incompatible with GPU execution;\n",
    "- (optional) use lower-level, CUDA-specific interfaces to implement missing functionality or optimize performance.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: GPU-port the `sqrt_sum` function we saw in te first notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function sqrt_sum(A)\n",
    "    T = eltype(A)\n",
    "    s = zero(T)\n",
    "    for i in eachindex(A)\n",
    "        @inbounds s += sqrt(A[i])\n",
    "    end\n",
    "    return s\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "- https://enccs.github.io/Julia-for-HPC/GPU/\n",
    "- https://cuda.juliagpu.org/stable/\n",
    "- https://www.youtube.com/watch?v=Fz-ogmASMAE\n",
    "- https://www.cherryservers.com/blog/gpu-vs-cpu-what-are-the-key-differences\n",
    "- https://developer.nvidia.com/blog/tag/cuda-refresher/\n",
    "- https://i.redd.it/yr9h5cpyzpn21.jpg\n",
    "- https://docs.nvidia.com/cuda/\n",
    "- https://www.youtube.com/watch?v=Hz9IMJuW5hU\n",
    "- https://julialang.org/learning/"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.10.1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
